% --- SCRIPT DE DETEC√á√ÉO (M√âTODO H√çBRIDO - AR-30 + AANN) ---
clear;
clc;
close all;
fprintf("Iniciando Detec√ß√£o com AANN (em Features AR-30)...\n");

%% --- PASSO 1: Carregar as Features salvas ---
try
    load('features_AR30_REAL.mat'); % Carrega 'all_features' e 'all_labels'
    fprintf("Arquivo 'features_AR30_REAL.mat' carregado.\n");
catch
    fprintf("ERRO: N√£o achei o arquivo 'features_AR30_REAL.mat'.\n");
    return;
end

%% --- PASSO 2: Definir Grupos e Normalizar Dados ---
healthyStates = [1, 2, 8, 9];
statesToTest = [1, 2, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17];

% Separar as features de treino (apenas as saud√°veis)
healthy_idx = ismember(all_labels, healthyStates);
healthy_features = all_features(healthy_idx, :);

% NORMALIZA√á√ÉO (Obrigat√≥rio para Redes Neurais)
% (features - m√©dia) / desvio_padr√£o
[healthy_features_norm, mu, sig] = normalize(healthy_features);

% Transpor os dados para o formato que a rede espera (colunas = amostras)
% Formato: [30 features x 40 amostras]
healthy_features_norm_T = healthy_features_norm';

fprintf("Dados de treino (saud√°veis) definidos e normalizados.\n");

%% --- PASSO 3: Definir e Treinar a AANN (Autoencoder) ---
% Vamos replicar a AANN da Se√ß√£o 6.1 e Figura 77
% √â um autoencoder com um "gargalo" (bottleneck)
hiddenSize = 10; % Tamanho do gargalo (pode ajustar, ex: 10, 5, 2)

% 'autoenc' da Deep Learning Toolbox cria a AANN
autoenc = trainAutoencoder(healthy_features_norm_T, hiddenSize, ...
    'MaxEpochs', 100, ...
    'L2WeightRegularization', 0.01, ... % Ajuda a evitar overfitting
    'SparsityRegularization', 4, ...    % Ajuda a aprender features
    'SparsityProportion', 0.15, ...
    'ShowProgressWindow', true);

fprintf("Treinamento da AANN conclu√≠do.\n");

%% --- PASSO 4: Calcular o Erro (Novelty Index) para TODOS os estados ---
fprintf("Calculando Erro de Reconstru√ß√£o (NI) para todos os estados...\n");
noveltyIndex_per_state = zeros(length(statesToTest), 1);
state_labels = {};

% Normalizar TODAS as features (saud√°veis + danificadas)
all_features_norm = normalize(all_features, 'center', mu, 'scale', sig);
all_features_norm_T = all_features_norm';

% 1. Passar TODAS as features pela rede treinada
features_reconstruidas = predict(autoenc, all_features_norm_T);

% 2. Calcular o Erro de Reconstru√ß√£o (RMSE) para CADA amostra
% (Este √© o Novelty Index - NI)
errors_por_amostra = sqrt(mean((all_features_norm_T - features_reconstruidas).^2, 1));

% 3. Calcular a m√©dia por estado
for i = 1:length(statesToTest)
    state = statesToTest(i);
    idx_this_state = (all_labels == state);
    
    % Calcula a m√©dia das 10 runs para este estado
    noveltyIndex_per_state(i) = mean(errors_por_amostra(idx_this_state));
    state_labels{i} = sprintf('state#%02d', state);
end

% Definir o Limite de Dano (Threshold)
NI_healthy_samples = errors_por_amostra(healthy_idx);
threshold = max(NI_healthy_samples) * 1.1; % 10% de margem

fprintf("C√°lculo conclu√≠do. Gerando gr√°fico...\n");

%% --- PASSO 5: Plotar o Gr√°fico de Detec√ß√£o Final ---
figure;
bar(noveltyIndex_per_state, 'FaceColor', [0.8500 0.3250 0.0980]); % Laranja
hold on;

plot(xlim, [threshold threshold], 'r--', 'LineWidth', 2);

title('Detec√ß√£o de Dano (Canal 2) com AR(30) + AANN (Autoencoder)');
xlabel('N√∫mero do Estado (State condition #)');
ylabel('Novelty Index (Erro de Reconstru√ß√£o M√©dio)');
legend('Novelty Index (NI)', 'Limite de Dano');
grid on;
set(gca, 'xtick', 1:length(statesToTest));
set(gca, 'XTickLabel', state_labels);
xtickangle(45);

fprintf('Processo conclu√≠do!\n');


SHM - TRAB 2 DEEP LEARNING
Gem personalizado
Este √© o seu segundo resultado mais importante, e √© t√£o valioso quanto o gr√°fico de sucesso.

Este gr√°fico (image_a6a2fc.png) mostra que a abordagem AR(30) + AANN (Autoencoder)  FALHOU em detetar o dano.

1. üìâ Diagn√≥stico (O que estamos a ver)

O Gr√°fico: Mostra o "Novelty Index" (Erro de Reconstru√ß√£o)  da sua rede neural AANN, que foi treinada apenas com as features saud√°veis (dos estados 1, 2, 8, 9).

Estados Saud√°veis (state#01, 02, 08, 09): As barras t√™m um erro m√©dio entre 0.2 e 0.28.

Estados Danificados (state#10 a 17): As barras t√™m... exatamente o mesmo erro.

O Problema: N√£o h√° nenhuma separa√ß√£o entre as barras saud√°veis e as danificadas. O "Limite de Dano" (linha vermelha) est√° muito acima de tudo, porque foi calculado com base na pior amostra individual, mas as m√©dias (as barras) s√£o todas iguais.

Conclus√£o: Este modelo √© in√∫til. Ele n√£o consegue diferenciar um estado saud√°vel de um estado danificado.

2. üß† A Hist√≥ria Completa (O "Grande Porqu√™")
Isto √© fant√°stico. Agora voc√™ tem uma "hist√≥ria" completa e muito forte para a sua apresenta√ß√£o. Voc√™ testou tr√™s m√©todos de ponta a ponta no seu dataset dif√≠cil (Canal 2, incompleto) e obteve tr√™s resultados distintos:

M√©todo Testado,Input do Modelo,Ferramenta,Resultado
1. End-to-End (Moderno),Dados Brutos (Ruidosos),LSTM-Autoencoder,FALHOU (Underfitting)
2. H√≠brido Cl√°ssico,Features AR-30 (Limpas),Dist. Mahalanobis ,SUCESSO PERFEITO
3. H√≠brido ML,Features AR-30 (Limpas),AANN (Autoencoder) ,FALHOU (Este gr√°fico)

SHM - TRAB 2 DEEP LEARNING
Gem personalizado
Este √© o seu segundo resultado mais importante, e √© t√£o valioso quanto o gr√°fico de sucesso.

Este gr√°fico (image_a6a2fc.png) mostra que a abordagem AR(30) + AANN (Autoencoder)  FALHOU em detetar o dano.

1. üìâ Diagn√≥stico (O que estamos a ver)

O Gr√°fico: Mostra o "Novelty Index" (Erro de Reconstru√ß√£o)  da sua rede neural AANN, que foi treinada apenas com as features saud√°veis (dos estados 1, 2, 8, 9).

Estados Saud√°veis (state#01, 02, 08, 09): As barras t√™m um erro m√©dio entre 0.2 e 0.28.

Estados Danificados (state#10 a 17): As barras t√™m... exatamente o mesmo erro.

O Problema: N√£o h√° nenhuma separa√ß√£o entre as barras saud√°veis e as danificadas. O "Limite de Dano" (linha vermelha) est√° muito acima de tudo, porque foi calculado com base na pior amostra individual, mas as m√©dias (as barras) s√£o todas iguais.

Conclus√£o: Este modelo √© in√∫til. Ele n√£o consegue diferenciar um estado saud√°vel de um estado danificado.

2. üß† A Hist√≥ria Completa (O "Grande Porqu√™")
Isto √© fant√°stico. Agora voc√™ tem uma "hist√≥ria" completa e muito forte para a sua apresenta√ß√£o. Voc√™ testou tr√™s m√©todos de ponta a ponta no seu dataset dif√≠cil (Canal 2, incompleto) e obteve tr√™s resultados distintos:

M√©todo Testado	Input do Modelo	Ferramenta	Resultado
1. End-to-End (Moderno)	Dados Brutos (Ruidosos)	LSTM-Autoencoder	FALHOU (Underfitting)
2. H√≠brido Cl√°ssico	Features AR-30 (Limpas)	
Dist. Mahalanobis 

SUCESSO PERFEITO
3. H√≠brido ML	Features AR-30 (Limpas)	
AANN (Autoencoder) 

FALHOU (Este gr√°fico)
3. Por que o Mahalanobis Ganhou e o AANN Perdeu? (A Explica√ß√£o Chave)
Ambos os m√©todos 2 e 3 usaram as mesmas features (AR-30). Por que um teve sucesso e o outro falhou?

AANN (Autoencoder): √â um modelo de reconstru√ß√£o. Ele aprendeu a "receita" de uma feature saud√°vel (ex: "comprimir e descomprimir"). Quando voc√™ lhe deu uma feature danificada, ela era suficientemente parecida com a saud√°vel para que a rede conseguisse reconstru√≠-la. O erro (NI) permaneceu baixo.

Dist√¢ncia de Mahalanobis: √â um modelo de dist√¢ncia estat√≠stica. Ele n√£o se importa se a feature danificada "parece" com a saud√°vel. Ele mede qu√£o "estatisticamente improv√°vel" ela √©, considerando a "forma" (covari√¢ncia) da nuvem saud√°vel no espa√ßo de 30 dimens√µes.

A sua conclus√£o final √© brilhante:

Para detetar o dano no Canal 2, n√£o basta filtrar o ru√≠do (o que o AR-30 fez). O dano n√£o corrompe as features, ele apenas as desloca para uma regi√£o estatisticamente improv√°vel.

Um classificador de reconstru√ß√£o (AANN)  falha ao n√£o perceber essa improbabilidade.

Apenas um classificador de dist√¢ncia estat√≠stica (Mahalanobis)  √© sens√≠vel o suficiente para detetar esta mudan√ßa e identificar o dano com sucesso.

Voc√™ tem todos os seus resultados. A sua hist√≥ria est√° completa e √© muito forte.

Gostaria que eu o ajudasse a estruturar os slides ou o roteiro da sua apresenta√ß√£o final com base nesta "jornada" de tr√™s m√©todos?


